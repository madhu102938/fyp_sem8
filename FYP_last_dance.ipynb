{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4a1d9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "971c2aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-06 18:30:32.659616: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-06 18:30:32.710958: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-06 18:30:32.711012: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-06 18:30:32.711056: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-06 18:30:32.721591: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-06 18:30:33.723969: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, losses, optimizers, Model\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80939d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_MCI_Features_file_path = r'./feature_extraction/MCI_func_features.pkl'\n",
    "with open(pickle_MCI_Features_file_path, 'rb') as file:\n",
    "    MCI_funct_features = pickle.load(file)\n",
    "MCI_funct_features.shape\n",
    "MCI_funct_features = np.array(MCI_funct_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c813465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 13456)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MCI_funct_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee3a6fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_MCI_Features_file_path = r'feature_extraction/cat12_MCI_structural_features.pkl'\n",
    "with open(pickle_MCI_Features_file_path, 'rb') as file:\n",
    "    MCI_Struct_features = pickle.load(file)\n",
    "MCI_Struct_features.shape\n",
    "MCI_Struct_features = np.array(MCI_Struct_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9886dee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 169, 205, 169)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MCI_Struct_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53ad34c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "structural_features loaded successfully!\n",
      "Shape of loaded array: (40, 169, 205, 169)\n"
     ]
    }
   ],
   "source": [
    "# Define the same file path\n",
    "file_path = r'feature_extraction/cat12_AD_structural_features.pkl'\n",
    "\n",
    "# Load the NumPy array from the file\n",
    "with open(file_path, \"rb\") as file:  # 'rb' means read binary\n",
    "    AD_struct_features = pickle.load(file)\n",
    "\n",
    "print(\"structural_features loaded successfully!\")\n",
    "print(\"Shape of loaded array:\", AD_struct_features.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "959837d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 13456)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickle_AD_Features_file_path = r\"feature_extraction/AD_func_features.pkl\"\n",
    "with open(pickle_AD_Features_file_path, 'rb') as file:\n",
    "    AD_funct_features = pickle.load(file)\n",
    "AD_funct_features = np.array(AD_funct_features)\n",
    "AD_funct_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "697e6519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "structural_features loaded successfully!\n",
      "Shape of loaded array: (30, 169, 205, 169)\n"
     ]
    }
   ],
   "source": [
    "# Define the same file path\n",
    "file_path = r'feature_extraction/cat12_CN_structural_features.pkl'\n",
    "\n",
    "# Load the NumPy array from the file\n",
    "with open(file_path, \"rb\") as file:  # 'rb' means read binary\n",
    "    CN_struct_features = pickle.load(file)\n",
    "\n",
    "print(\"structural_features loaded successfully!\")\n",
    "print(\"Shape of loaded array:\", CN_struct_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9217afde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 13456)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickle_AD_Features_file_path = r\"feature_extraction/CN_func_features.pkl\"\n",
    "with open(pickle_AD_Features_file_path, 'rb') as file:\n",
    "    CN_funct_features = pickle.load(file)\n",
    "CN_funct_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4526fd16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((52, 13456), (52, 169, 205, 169))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MCI_funct_features.shape, MCI_Struct_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed6ff037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40, 13456), (40, 169, 205, 169))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AD_funct_features.shape, AD_struct_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ac8adef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30, 13456), (30, 169, 205, 169))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CN_funct_features.shape, CN_struct_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4e4b395",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data Shapes\n",
    "AD_Struct_features = AD_struct_features\n",
    "# Labels\n",
    "MCI_labels = np.zeros(MCI_funct_features.shape[0])  # Label 0 for MCI\n",
    "AD_labels = np.ones(AD_funct_features.shape[0])    # Label 1 for AD\n",
    "CN_labels = np.array([2]*CN_funct_features.shape[0])\n",
    "\n",
    "# Combine functional and structural features\n",
    "funct_features = np.vstack((MCI_funct_features, AD_funct_features, CN_funct_features))\n",
    "struct_features = np.vstack((MCI_Struct_features, AD_Struct_features, CN_struct_features))\n",
    "labels = np.hstack((MCI_labels, AD_labels, CN_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8cce47cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "struct_features = np.expand_dims(struct_features, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "424a6930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(122, 116, 116, 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "funct_features = funct_features.reshape(-1, 116, 116)\n",
    "funct_features = np.expand_dims(funct_features, axis=-1)\n",
    "funct_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99402c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "794478e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train_3d, X_test_3d, X_train_2d, X_test_2d, y_train, y_test = train_test_split(\n",
    "    struct_features,\n",
    "    funct_features,\n",
    "    labels,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "498c87b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator labels (real images are labeled as 1)\n",
    "real_labels_train = np.ones((X_train_3d.shape[0], 1))\n",
    "real_labels_test = np.ones((X_test_3d.shape[0], 1))\n",
    "\n",
    "# For simplicity, use real labels for reconstructed images\n",
    "disc_targets_3d_train = real_labels_train\n",
    "disc_targets_2d_train = real_labels_train\n",
    "\n",
    "disc_targets_3d_test = real_labels_test\n",
    "disc_targets_2d_test = real_labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aec97d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, Input\n",
    "\n",
    "def build_encoder_3d(input_shape):\n",
    "    inputs = Input(shape=input_shape) # 169x205x169x1 (input, 1 channel)\n",
    "    x = layers.Conv3D(32, (3, 3, 3), activation='gelu', padding='valid')(inputs) # 166×203×166 x32\n",
    "    x = layers.MaxPooling3D((2, 2, 2))(x) # 83×101×83 x32\n",
    "    x = layers.Conv3D(64, (3, 3, 3), activation='gelu', padding='valid')(x) # 81×99×81 x64\n",
    "    x = layers.MaxPooling3D((2, 2, 2))(x) # 40×49×40 x64\n",
    "    x = layers.Conv3D(128, (3, 3, 3), activation='gelu', padding='valid')(x) # 38×47×38 128\n",
    "    x = layers.Flatten()(x)\n",
    "    latent = layers.Dense(256, activation='gelu')(x)\n",
    "    encoder = Model(inputs, latent, name='encoder_3d')\n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "555b5c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_encoder_2d(input_shape):\n",
    "    inputs = Input(shape=input_shape) # 116x116 x1\n",
    "    x = layers.Conv2D(32, (3, 3), activation='gelu', padding='valid')(inputs) # 114x114 x32\n",
    "    x = layers.MaxPooling2D((2, 2))(x) # 57x57 x32\n",
    "    x = layers.Conv2D(64, (3, 3), activation='gelu', padding='valid')(x) # 55x55 x64\n",
    "    x = layers.MaxPooling2D((2, 2))(x) # 27x27 x64\n",
    "    x = layers.Conv2D(128, (3, 3), activation='gelu', padding='valid')(x) # 25x25 x128\n",
    "    x = layers.Flatten()(x)\n",
    "    latent = layers.Dense(256, activation='gelu')(x)\n",
    "    encoder = Model(inputs, latent, name='encoder_2d')\n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0084b724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_decoder_3d(latent_dim, output_shape):\n",
    "    latent_inputs = Input(shape=(latent_dim,))\n",
    "    reshape_shape = np.array([output_shape[0]//8, output_shape[1]//8, output_shape[2]//8, 128])\n",
    "    x = layers.Dense(np.prod(reshape_shape), activation='gelu')(latent_inputs)\n",
    "    x = layers.Reshape(reshape_shape)(x)\n",
    "    x = layers.Conv3DTranspose(128, (3, 3, 3), strides=2, padding='valid', activation='gelu')(x)\n",
    "    x = layers.Conv3DTranspose(64, (3, 3, 3), strides=2, padding='valid', activation='gelu')(x)\n",
    "    x = layers.Conv3DTranspose(32, (3, 3, 3), strides=2, padding='valid', activation='gelu')(x)\n",
    "    # Adjust the final Conv3DTranspose layer to match the expected output shape\n",
    "    x = layers.Conv3DTranspose(1, (3, 3, 3), strides=(1, 1, 1), padding='valid', activation='sigmoid')(x)\n",
    "    # x = layers.ZeroPadding3D(padding=((0, 1), (0, 5), (0, 1)))(x)  # Add padding to match (169, 205, 169)    \n",
    "    x = layers.Cropping3D(cropping=((4, 4), (2, 2), (4, 4)))(x)\n",
    "    outputs = x\n",
    "    \n",
    "    # Ensure the output shape matches the input shape of the discriminator\n",
    "    assert outputs.shape[1:] == output_shape, f\"Output shape {outputs.shape[1:]} does not match expected shape {output_shape}\"\n",
    "    \n",
    "    decoder = Model(latent_inputs, outputs, name='decoder_3d')\n",
    "    return decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "182bf318",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_decoder_2d(latent_dim, output_shape):\n",
    "    latent_inputs = Input(shape=(latent_dim,))\n",
    "    x = layers.Dense((output_shape[0]//4)*(output_shape[1]//4)*64, activation='gelu')(latent_inputs)\n",
    "    x = layers.Reshape((output_shape[0]//4, output_shape[1]//4, 64))(x)\n",
    "    x = layers.Conv2DTranspose(128, (3, 3), strides=2, padding='valid', activation='gelu')(x)\n",
    "    x = layers.Conv2DTranspose(64, (3, 3), strides=2, padding='valid', activation='gelu')(x)\n",
    "    x = layers.Conv2DTranspose(1, (3, 3), padding='valid', activation='sigmoid')(x)\n",
    "    outputs = layers.Cropping2D(cropping=((2, 3), (2, 3)))(x) \n",
    "\n",
    "    assert outputs.shape[1:-1] == output_shape, f\"Output shape {outputs.shape[1:-1]} does not match expected shape {output_shape}\"\n",
    "\n",
    "\n",
    "    decoder = Model(latent_inputs, outputs, name='decoder_2d')\n",
    "    return decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fa84d0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator_3d(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = layers.Conv3D(32, (3, 3, 3), activation='gelu')(inputs)\n",
    "    x = layers.MaxPooling3D((2, 2, 2))(x)\n",
    "    x = layers.Conv3D(64, (3, 3, 3), activation='gelu')(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "    discriminator = Model(inputs, outputs, name='discriminator_3d')\n",
    "    return discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7b00762d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator_2d(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = layers.Conv2D(32, (3, 3), activation='gelu')(inputs)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Conv2D(64, (3, 3), activation='gelu')(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "    discriminator = Model(inputs, outputs, name='discriminator_2d')\n",
    "    return discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "947f1e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier(input_dim, num_classes):\n",
    "    inputs = Input(shape=(input_dim,))\n",
    "    x = layers.Dense(256, activation='gelu')(inputs)\n",
    "    x = layers.Dense(128, activation='gelu')(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    classifier = Model(inputs, outputs, name='classifier')\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ffb776c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input shapes\n",
    "input_shape_3d = (169, 205, 169, 1)\n",
    "input_shape_2d = (116, 116, 1)\n",
    "latent_dim = 256\n",
    "num_classes = 3\n",
    "\n",
    "# Build models\n",
    "encoder_3d = build_encoder_3d(input_shape_3d)\n",
    "encoder_2d = build_encoder_2d(input_shape_2d)\n",
    "decoder_3d = build_decoder_3d(latent_dim, input_shape_3d)\n",
    "decoder_2d = build_decoder_2d(latent_dim, input_shape_2d[:-1])\n",
    "discriminator_3d = build_discriminator_3d(input_shape_3d)\n",
    "discriminator_2d = build_discriminator_2d(input_shape_2d)\n",
    "# Build classifier that accepts concatenated latent vectors\n",
    "classifier = build_classifier(latent_dim * 2, num_classes)\n",
    "\n",
    "# Inputs\n",
    "input_3d = Input(shape=input_shape_3d)\n",
    "input_2d = Input(shape=input_shape_2d)\n",
    "\n",
    "# Encoding\n",
    "latent_3d = encoder_3d(input_3d)\n",
    "latent_2d = encoder_2d(input_2d)\n",
    "\n",
    "# Concatenate the two latent representations\n",
    "combined_latent = layers.Concatenate()([latent_3d, latent_2d])\n",
    "\n",
    "# Decoding\n",
    "reconstructed_3d = decoder_3d(latent_3d)\n",
    "reconstructed_2d = decoder_2d(latent_2d)\n",
    "\n",
    "# Discriminator outputs\n",
    "disc_output_3d = discriminator_3d(reconstructed_3d)\n",
    "disc_output_2d = discriminator_2d(reconstructed_2d)\n",
    "\n",
    "# Get classification output\n",
    "classification_output = classifier(combined_latent)\n",
    "\n",
    "# Define the combined model\n",
    "model = Model(\n",
    "    inputs=[input_3d, input_2d],\n",
    "    outputs=[\n",
    "        reconstructed_3d,\n",
    "        reconstructed_2d,\n",
    "        disc_output_3d,\n",
    "        disc_output_2d,\n",
    "        classification_output\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2fa41a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss weights\n",
    "lambda_reconstruction = 1.0\n",
    "lambda_adversarial = 0.1\n",
    "lambda_classification = 1.0\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss={\n",
    "        'decoder_3d': 'mse',\n",
    "        'decoder_2d': 'mse',\n",
    "        'discriminator_3d': tf.keras.losses.BinaryCrossentropy(),\n",
    "        'discriminator_2d': tf.keras.losses.BinaryCrossentropy(),\n",
    "        'classifier': 'sparse_categorical_crossentropy'\n",
    "    },\n",
    "    loss_weights={\n",
    "        'decoder_3d': lambda_reconstruction,\n",
    "        'decoder_2d': lambda_reconstruction,\n",
    "        'discriminator_3d': lambda_adversarial,\n",
    "        'discriminator_2d': lambda_adversarial,\n",
    "        'classifier': lambda_classification\n",
    "    },\n",
    "    metrics={\n",
    "        'classifier': 'accuracy'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f2bceba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator labels (real images are labeled as 1)\n",
    "real_labels = np.ones((struct_features.shape[0], 1))\n",
    "fake_labels = np.zeros((struct_features.shape[0], 1))\n",
    "\n",
    "# For simplicity, use real labels for reconstructed images (you can adjust as needed)\n",
    "disc_targets_3d = real_labels\n",
    "disc_targets_2d = real_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "71fb2286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-06 18:32:11.367970: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x73b8ac00bc50 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2024-12-06 18:32:11.368116: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2024-12-06 18:32:11.413056: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-12-06 18:32:12.667175: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2024-12-06 18:32:12.669886: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2024-12-06 18:32:12.785374: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2024-12-06 18:33:08.200004: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2024-12-06 18:33:08.224766: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2024-12-06 18:33:08.292487: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/13 [======================>.......] - ETA: 1:01 - loss: 36.7330 - decoder_3d_loss: 0.1381 - decoder_2d_loss: 0.0650 - discriminator_3d_loss: 0.0684 - discriminator_2d_loss: 0.0771 - classifier_loss: 36.5153 - classifier_accuracy: 0.4250"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-06 18:36:20.647754: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 317s 21s/step - loss: 32.3870 - decoder_3d_loss: 0.1188 - decoder_2d_loss: 0.0645 - discriminator_3d_loss: 0.0564 - discriminator_2d_loss: 0.0636 - classifier_loss: 32.1917 - classifier_accuracy: 0.3814 - val_loss: 11.8231 - val_decoder_3d_loss: 0.0232 - val_decoder_2d_loss: 0.0539 - val_discriminator_3d_loss: 0.0000e+00 - val_discriminator_2d_loss: 1.7584e-16 - val_classifier_loss: 11.7460 - val_classifier_accuracy: 0.4800\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 258s 20s/step - loss: 16.1069 - decoder_3d_loss: 0.0457 - decoder_2d_loss: 0.0512 - discriminator_3d_loss: 0.0000e+00 - discriminator_2d_loss: 5.0646e-17 - classifier_loss: 16.0100 - classifier_accuracy: 0.3608 - val_loss: 21.5787 - val_decoder_3d_loss: 0.0207 - val_decoder_2d_loss: 0.0470 - val_discriminator_3d_loss: 0.0000e+00 - val_discriminator_2d_loss: 1.8128e-24 - val_classifier_loss: 21.5110 - val_classifier_accuracy: 0.4800\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 260s 20s/step - loss: 13.9088 - decoder_3d_loss: 0.0193 - decoder_2d_loss: 0.0494 - discriminator_3d_loss: 0.0000e+00 - discriminator_2d_loss: 2.6770e-23 - classifier_loss: 13.8402 - classifier_accuracy: 0.2784 - val_loss: 40.6796 - val_decoder_3d_loss: 0.0118 - val_decoder_2d_loss: 0.0530 - val_discriminator_3d_loss: 0.0000e+00 - val_discriminator_2d_loss: 1.3870e-24 - val_classifier_loss: 40.6148 - val_classifier_accuracy: 0.3200\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 259s 20s/step - loss: 14.0527 - decoder_3d_loss: 0.0152 - decoder_2d_loss: 0.0565 - discriminator_3d_loss: 0.0000e+00 - discriminator_2d_loss: 3.8408e-25 - classifier_loss: 13.9810 - classifier_accuracy: 0.3918 - val_loss: 8.9735 - val_decoder_3d_loss: 0.0122 - val_decoder_2d_loss: 0.0449 - val_discriminator_3d_loss: 0.0000e+00 - val_discriminator_2d_loss: 2.4932e-24 - val_classifier_loss: 8.9164 - val_classifier_accuracy: 0.4800\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 255s 20s/step - loss: 13.8663 - decoder_3d_loss: 0.0115 - decoder_2d_loss: 0.0523 - discriminator_3d_loss: 0.0000e+00 - discriminator_2d_loss: 1.0442e-16 - classifier_loss: 13.8025 - classifier_accuracy: 0.3196 - val_loss: 5.8336 - val_decoder_3d_loss: 0.0077 - val_decoder_2d_loss: 0.0510 - val_discriminator_3d_loss: 0.0000e+00 - val_discriminator_2d_loss: 3.3026e-22 - val_classifier_loss: 5.7749 - val_classifier_accuracy: 0.4800\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 260s 20s/step - loss: 5.7902 - decoder_3d_loss: 0.0072 - decoder_2d_loss: 0.0530 - discriminator_3d_loss: 0.0000e+00 - discriminator_2d_loss: 1.3900e-22 - classifier_loss: 5.7300 - classifier_accuracy: 0.2887 - val_loss: 19.0844 - val_decoder_3d_loss: 0.0135 - val_decoder_2d_loss: 0.0596 - val_discriminator_3d_loss: 0.0000e+00 - val_discriminator_2d_loss: 2.8151e-28 - val_classifier_loss: 19.0113 - val_classifier_accuracy: 0.2000\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 255s 20s/step - loss: 9.1971 - decoder_3d_loss: 0.0098 - decoder_2d_loss: 0.0486 - discriminator_3d_loss: 0.0000e+00 - discriminator_2d_loss: 3.1179e-26 - classifier_loss: 9.1387 - classifier_accuracy: 0.3505 - val_loss: 4.0763 - val_decoder_3d_loss: 0.0063 - val_decoder_2d_loss: 0.0465 - val_discriminator_3d_loss: 0.0000e+00 - val_discriminator_2d_loss: 7.9042e-24 - val_classifier_loss: 4.0235 - val_classifier_accuracy: 0.4800\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 256s 20s/step - loss: 1.9433 - decoder_3d_loss: 0.0062 - decoder_2d_loss: 0.0490 - discriminator_3d_loss: 0.0000e+00 - discriminator_2d_loss: 4.1723e-18 - classifier_loss: 1.8881 - classifier_accuracy: 0.4433 - val_loss: 5.1606 - val_decoder_3d_loss: 0.0053 - val_decoder_2d_loss: 0.0418 - val_discriminator_3d_loss: 0.0000e+00 - val_discriminator_2d_loss: 1.9141e-20 - val_classifier_loss: 5.1135 - val_classifier_accuracy: 0.3200\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 257s 20s/step - loss: 2.4042 - decoder_3d_loss: 0.0050 - decoder_2d_loss: 0.0405 - discriminator_3d_loss: 0.0000e+00 - discriminator_2d_loss: 2.1778e-16 - classifier_loss: 2.3587 - classifier_accuracy: 0.3608 - val_loss: 1.1831 - val_decoder_3d_loss: 0.0048 - val_decoder_2d_loss: 0.0361 - val_discriminator_3d_loss: 0.0000e+00 - val_discriminator_2d_loss: 4.4283e-22 - val_classifier_loss: 1.1422 - val_classifier_accuracy: 0.4800\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 253s 19s/step - loss: 1.0649 - decoder_3d_loss: 0.0049 - decoder_2d_loss: 0.0381 - discriminator_3d_loss: 0.0000e+00 - discriminator_2d_loss: 2.4884e-16 - classifier_loss: 1.0220 - classifier_accuracy: 0.5052 - val_loss: 1.1749 - val_decoder_3d_loss: 0.0047 - val_decoder_2d_loss: 0.0364 - val_discriminator_3d_loss: 0.0000e+00 - val_discriminator_2d_loss: 2.8967e-19 - val_classifier_loss: 1.1338 - val_classifier_accuracy: 0.5200\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 253s 19s/step - loss: 0.9981 - decoder_3d_loss: 0.0045 - decoder_2d_loss: 0.0328 - discriminator_3d_loss: 0.0000e+00 - discriminator_2d_loss: 1.4987e-16 - classifier_loss: 0.9608 - classifier_accuracy: 0.6186 - val_loss: 0.9833 - val_decoder_3d_loss: 0.0044 - val_decoder_2d_loss: 0.0325 - val_discriminator_3d_loss: 0.0000e+00 - val_discriminator_2d_loss: 3.2197e-19 - val_classifier_loss: 0.9464 - val_classifier_accuracy: 0.5600\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 252s 19s/step - loss: 0.7315 - decoder_3d_loss: 0.0042 - decoder_2d_loss: 0.0293 - discriminator_3d_loss: 0.0000e+00 - discriminator_2d_loss: 8.6640e-20 - classifier_loss: 0.6980 - classifier_accuracy: 0.6701 - val_loss: 1.5843 - val_decoder_3d_loss: 0.0039 - val_decoder_2d_loss: 0.0321 - val_discriminator_3d_loss: 0.0000e+00 - val_discriminator_2d_loss: 4.7011e-21 - val_classifier_loss: 1.5483 - val_classifier_accuracy: 0.4400\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 252s 19s/step - loss: 0.6595 - decoder_3d_loss: 0.0040 - decoder_2d_loss: 0.0315 - discriminator_3d_loss: 0.0000e+00 - discriminator_2d_loss: 4.8631e-18 - classifier_loss: 0.6240 - classifier_accuracy: 0.7423 - val_loss: 1.1112 - val_decoder_3d_loss: 0.0038 - val_decoder_2d_loss: 0.0334 - val_discriminator_3d_loss: 0.0000e+00 - val_discriminator_2d_loss: 2.5430e-21 - val_classifier_loss: 1.0740 - val_classifier_accuracy: 0.4800\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 251s 19s/step - loss: 0.4326 - decoder_3d_loss: 0.0039 - decoder_2d_loss: 0.0316 - discriminator_3d_loss: 0.0000e+00 - discriminator_2d_loss: 1.2683e-15 - classifier_loss: 0.3972 - classifier_accuracy: 0.8454 - val_loss: 1.0747 - val_decoder_3d_loss: 0.0037 - val_decoder_2d_loss: 0.0344 - val_discriminator_3d_loss: 0.0000e+00 - val_discriminator_2d_loss: 6.0086e-15 - val_classifier_loss: 1.0366 - val_classifier_accuracy: 0.5600\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 253s 20s/step - loss: 0.2431 - decoder_3d_loss: 0.0037 - decoder_2d_loss: 0.0304 - discriminator_3d_loss: 0.0000e+00 - discriminator_2d_loss: 6.6260e-15 - classifier_loss: 0.2090 - classifier_accuracy: 0.9485 - val_loss: 1.2599 - val_decoder_3d_loss: 0.0036 - val_decoder_2d_loss: 0.0320 - val_discriminator_3d_loss: 0.0000e+00 - val_discriminator_2d_loss: 2.6296e-23 - val_classifier_loss: 1.2243 - val_classifier_accuracy: 0.6000\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 252s 19s/step - loss: 0.1386 - decoder_3d_loss: 0.0037 - decoder_2d_loss: 0.0283 - discriminator_3d_loss: 0.0000e+00 - discriminator_2d_loss: 4.8960e-14 - classifier_loss: 0.1066 - classifier_accuracy: 0.9897 - val_loss: 1.9775 - val_decoder_3d_loss: 0.0036 - val_decoder_2d_loss: 0.0309 - val_discriminator_3d_loss: 0.0000e+00 - val_discriminator_2d_loss: 1.1592e-17 - val_classifier_loss: 1.9430 - val_classifier_accuracy: 0.5200\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 253s 20s/step - loss: 0.0920 - decoder_3d_loss: 0.0037 - decoder_2d_loss: 0.0262 - discriminator_3d_loss: 0.0000e+00 - discriminator_2d_loss: 2.6987e-19 - classifier_loss: 0.0621 - classifier_accuracy: 0.9794 - val_loss: 1.7137 - val_decoder_3d_loss: 0.0036 - val_decoder_2d_loss: 0.0297 - val_discriminator_3d_loss: 0.0000e+00 - val_discriminator_2d_loss: 1.4131e-19 - val_classifier_loss: 1.6804 - val_classifier_accuracy: 0.6000\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 251s 19s/step - loss: 0.1703 - decoder_3d_loss: 0.0037 - decoder_2d_loss: 0.0268 - discriminator_3d_loss: 0.0000e+00 - discriminator_2d_loss: 2.2879e-15 - classifier_loss: 0.1398 - classifier_accuracy: 0.9588 - val_loss: 1.2403 - val_decoder_3d_loss: 0.0036 - val_decoder_2d_loss: 0.0312 - val_discriminator_3d_loss: 0.0000e+00 - val_discriminator_2d_loss: 1.0952e-19 - val_classifier_loss: 1.2055 - val_classifier_accuracy: 0.5600\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 251s 19s/step - loss: 0.1095 - decoder_3d_loss: 0.0036 - decoder_2d_loss: 0.0264 - discriminator_3d_loss: 0.0000e+00 - discriminator_2d_loss: 1.7093e-20 - classifier_loss: 0.0796 - classifier_accuracy: 0.9691 - val_loss: 2.9963 - val_decoder_3d_loss: 0.0035 - val_decoder_2d_loss: 0.0312 - val_discriminator_3d_loss: 0.0000e+00 - val_discriminator_2d_loss: 1.4455e-22 - val_classifier_loss: 2.9615 - val_classifier_accuracy: 0.4400\n",
      "Epoch 20/20\n",
      "12/13 [==========================>...] - ETA: 19s - loss: 0.1696 - decoder_3d_loss: 0.0036 - decoder_2d_loss: 0.0257 - discriminator_3d_loss: 0.0000e+00 - discriminator_2d_loss: 1.3582e-17 - classifier_loss: 0.1403 - classifier_accuracy: 0.9688"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-06 19:57:35.642589: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 252s 19s/step - loss: 0.1685 - decoder_3d_loss: 0.0036 - decoder_2d_loss: 0.0256 - discriminator_3d_loss: 0.0000e+00 - discriminator_2d_loss: 1.3442e-17 - classifier_loss: 0.1393 - classifier_accuracy: 0.9691 - val_loss: 2.3056 - val_decoder_3d_loss: 0.0035 - val_decoder_2d_loss: 0.0283 - val_discriminator_3d_loss: 0.0000e+00 - val_discriminator_2d_loss: 3.9778e-21 - val_classifier_loss: 2.2738 - val_classifier_accuracy: 0.4800\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    [X_train_3d, X_train_2d],\n",
    "    {\n",
    "        'decoder_3d': X_train_3d,\n",
    "        'decoder_2d': X_train_2d,\n",
    "        'discriminator_3d': disc_targets_3d_train,\n",
    "        'discriminator_2d': disc_targets_2d_train,\n",
    "        'classifier': y_train\n",
    "    },\n",
    "    epochs=20,\n",
    "    batch_size=8,\n",
    "    validation_data=(\n",
    "        [X_test_3d, X_test_2d],\n",
    "        {\n",
    "            'decoder_3d': X_test_3d,\n",
    "            'decoder_2d': X_test_2d,\n",
    "            'discriminator_3d': disc_targets_3d_test,\n",
    "            'discriminator_2d': disc_targets_2d_test,\n",
    "            'classifier': y_test\n",
    "        }\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a84aa083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Test Accuracy: 48.00%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the classifier accuracy on the test set\n",
    "evaluation = model.evaluate(\n",
    "    [X_test_3d, X_test_2d],\n",
    "    {\n",
    "        'decoder_3d': X_test_3d,\n",
    "        'decoder_2d': X_test_2d,\n",
    "        'discriminator_3d': disc_targets_3d_test,\n",
    "        'discriminator_2d': disc_targets_2d_test,\n",
    "        'classifier': y_test\n",
    "    },\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Get the index of the classification loss and accuracy (depends on the order of outputs)\n",
    "classification_accuracy = evaluation[model.metrics_names.index('classifier_accuracy')]\n",
    "\n",
    "print(f'Classifier Test Accuracy: {classification_accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1bf5dcce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Train Accuracy : 94.85\n"
     ]
    }
   ],
   "source": [
    "evaluation_train = model.evaluate(\n",
    "    [X_train_3d, X_train_2d],\n",
    "    {\n",
    "        'decoder_3d': X_train_3d,\n",
    "        'decoder_2d': X_train_2d,\n",
    "        'discriminator_3d': disc_targets_3d_train,\n",
    "        'discriminator_2d': disc_targets_2d_train,\n",
    "        'classifier': y_train\n",
    "    },\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "classification_accuracy = evaluation_train[model.metrics_names.index('classifier_accuracy')]\n",
    "\n",
    "print(f'Classifier Train Accuracy : {classification_accuracy * 100:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2e39325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "devices = tf.config.list_physical_devices()\n",
    "print(devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ded408c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "73cd664e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tripti/miniconda3/lib/python3.9/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] Can't close file (file write failed: time = Thu Nov 28 18:09:47 2024\n, filename = 'models/fusion_model_1.h5', file descriptor = 84, errno = 28, error message = 'No space left on device', buf = 0xbfd3120, total write size = 47824, bytes this sub-write = 47824, bytes actually written = 18446744073709551615, offset = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels/fusion_model_1.h5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/h5py/_hl/files.py:581\u001b[0m, in \u001b[0;36mFile.close\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid\u001b[38;5;241m.\u001b[39mvalid:\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;66;03m# We have to explicitly murder all open objects related to the file\u001b[39;00m\n\u001b[1;32m    577\u001b[0m \n\u001b[1;32m    578\u001b[0m     \u001b[38;5;66;03m# Close file-resident objects first, then the files.\u001b[39;00m\n\u001b[1;32m    579\u001b[0m     \u001b[38;5;66;03m# Otherwise we get errors in MPI mode.\u001b[39;00m\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid\u001b[38;5;241m.\u001b[39m_close_open_objects(h5f\u001b[38;5;241m.\u001b[39mOBJ_LOCAL \u001b[38;5;241m|\u001b[39m \u001b[38;5;241m~\u001b[39mh5f\u001b[38;5;241m.\u001b[39mOBJ_FILE)\n\u001b[0;32m--> 581\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid\u001b[38;5;241m.\u001b[39m_close_open_objects(h5f\u001b[38;5;241m.\u001b[39mOBJ_LOCAL \u001b[38;5;241m|\u001b[39m h5f\u001b[38;5;241m.\u001b[39mOBJ_FILE)\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    584\u001b[0m     _objects\u001b[38;5;241m.\u001b[39mnonlocal_close()\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5f.pyx:355\u001b[0m, in \u001b[0;36mh5py.h5f.FileID._close_open_objects\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] Can't close file (file write failed: time = Thu Nov 28 18:09:47 2024\n, filename = 'models/fusion_model_1.h5', file descriptor = 84, errno = 28, error message = 'No space left on device', buf = 0xbfd3120, total write size = 47824, bytes this sub-write = 47824, bytes actually written = 18446744073709551615, offset = 0)"
     ]
    }
   ],
   "source": [
    "model.save(\"models/fusion_model_1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603237ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
