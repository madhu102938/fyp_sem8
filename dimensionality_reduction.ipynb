{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mci_data = pickle.load(open(r'./feature_extraction/MCI_final_features_all_1.pkl', 'rb'))\n",
    "cn_data = np.array(pickle.load(open(r'./feature_extraction\\func_features_AD.pkl', 'rb')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mci_ad = np.append(mci_data, cn_data, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(93,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.zeros((mci_data.shape[0],))\n",
    "y = np.append(y, np.ones((cn_data.shape[0],)))\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mci_ad = np.nan_to_num(mci_ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((74, 13456), (19, 13456), (74,), (19,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test data train data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(mci_ad, y, test_size=0.2, random_state=42)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard scalar\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "mms = MinMaxScaler()\n",
    "X_train_scaled = mms.fit_transform(X_train)\n",
    "X_test_scaled = mms.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = X_train_scaled.shape[1]\n",
    "hidden_size = 5000\n",
    "hidden_size2 = 1000\n",
    "output_size = 700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "input_dim = X_train_scaled.shape[1]\n",
    "encoding_dim = 1024\n",
    "num_classes = len(set(y_train))  # Assuming y_train contains the class labels\n",
    "batch_size = 64\n",
    "epochs = 50\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the Autoencoder\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, encoding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(encoding_dim, encoding_dim // 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim // 2, encoding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(encoding_dim, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded, encoded\n",
    "\n",
    "# Define the Classifier\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, encoding_dim, num_classes):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(encoding_dim // 2, encoding_dim // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(encoding_dim // 4, num_classes),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the Autoencoder\n",
    "autoencoder = Autoencoder(input_dim=input_dim, encoding_dim=encoding_dim)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.06076010316610336\n",
      "Epoch 2, Loss: 0.05295880138874054\n",
      "Epoch 3, Loss: 0.04236038401722908\n",
      "Epoch 4, Loss: 0.04489137418568134\n",
      "Epoch 5, Loss: 0.04264094866812229\n",
      "Epoch 6, Loss: 0.04710724949836731\n",
      "Epoch 7, Loss: 0.04531878978013992\n",
      "Epoch 8, Loss: 0.04162396676838398\n",
      "Epoch 9, Loss: 0.04682427830994129\n",
      "Epoch 10, Loss: 0.04098748415708542\n",
      "Epoch 11, Loss: 0.04023466631770134\n",
      "Epoch 12, Loss: 0.04363626427948475\n",
      "Epoch 13, Loss: 0.04010681062936783\n",
      "Epoch 14, Loss: 0.03683747164905071\n",
      "Epoch 15, Loss: 0.03883066587150097\n",
      "Epoch 16, Loss: 0.03772849775850773\n",
      "Epoch 17, Loss: 0.035100373439490795\n",
      "Epoch 18, Loss: 0.03787737898528576\n",
      "Epoch 19, Loss: 0.039093900471925735\n",
      "Epoch 20, Loss: 0.034692125394940376\n",
      "Epoch 21, Loss: 0.030923940241336823\n",
      "Epoch 22, Loss: 0.0339828971773386\n",
      "Epoch 23, Loss: 0.035185279324650764\n",
      "Epoch 24, Loss: 0.03140402119606733\n",
      "Epoch 25, Loss: 0.031418521888554096\n",
      "Epoch 26, Loss: 0.035372812300920486\n",
      "Epoch 27, Loss: 0.029250643216073513\n",
      "Epoch 28, Loss: 0.027554603293538094\n",
      "Epoch 29, Loss: 0.03352508693933487\n",
      "Epoch 30, Loss: 0.027543246746063232\n",
      "Epoch 31, Loss: 0.02881352137774229\n",
      "Epoch 32, Loss: 0.029992152005434036\n",
      "Epoch 33, Loss: 0.029920552857220173\n",
      "Epoch 34, Loss: 0.027520902454853058\n",
      "Epoch 35, Loss: 0.028457043692469597\n",
      "Epoch 36, Loss: 0.026116903871297836\n",
      "Epoch 37, Loss: 0.0282853115350008\n",
      "Epoch 38, Loss: 0.025320705957710743\n",
      "Epoch 39, Loss: 0.02672579512000084\n",
      "Epoch 40, Loss: 0.02804571855813265\n",
      "Epoch 41, Loss: 0.026935814879834652\n",
      "Epoch 42, Loss: 0.028367817401885986\n",
      "Epoch 43, Loss: 0.025340253487229347\n",
      "Epoch 44, Loss: 0.02536324318498373\n",
      "Epoch 45, Loss: 0.025431931018829346\n",
      "Epoch 46, Loss: 0.025496207177639008\n",
      "Epoch 47, Loss: 0.024502648040652275\n",
      "Epoch 48, Loss: 0.024202650412917137\n",
      "Epoch 49, Loss: 0.025425689294934273\n",
      "Epoch 50, Loss: 0.02447192557156086\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the Autoencoder\n",
    "for epoch in range(epochs):\n",
    "    autoencoder.train()\n",
    "    train_loss = 0\n",
    "    for data, _ in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        reconstructed, _ = autoencoder(data)\n",
    "        loss = criterion(reconstructed, data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    print(f'Epoch {epoch+1}, Loss: {train_loss / len(train_loader)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract the encoded features\n",
    "autoencoder.eval()\n",
    "with torch.no_grad():\n",
    "    _, encoded_features = autoencoder(X_train_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the Classifier\n",
    "classifier = Classifier(encoding_dim=encoding_dim, num_classes=num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create DataLoader for encoded features\n",
    "encoded_dataset = TensorDataset(encoded_features, y_train_tensor)\n",
    "encoded_loader = DataLoader(encoded_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.706015944480896\n",
      "Epoch 2, Loss: 0.7093174755573273\n",
      "Epoch 3, Loss: 0.6833215355873108\n",
      "Epoch 4, Loss: 0.6833140850067139\n",
      "Epoch 5, Loss: 0.6615369617938995\n",
      "Epoch 6, Loss: 0.6567787826061249\n",
      "Epoch 7, Loss: 0.6348120868206024\n",
      "Epoch 8, Loss: 0.632046639919281\n",
      "Epoch 9, Loss: 0.6645532250404358\n",
      "Epoch 10, Loss: 0.6854847371578217\n",
      "Epoch 11, Loss: 0.6195466220378876\n",
      "Epoch 12, Loss: 0.6367692053318024\n",
      "Epoch 13, Loss: 0.6818309724330902\n",
      "Epoch 14, Loss: 0.647771418094635\n",
      "Epoch 15, Loss: 0.6743091642856598\n",
      "Epoch 16, Loss: 0.6734460890293121\n",
      "Epoch 17, Loss: 0.6076741516590118\n",
      "Epoch 18, Loss: 0.642526239156723\n",
      "Epoch 19, Loss: 0.6871246993541718\n",
      "Epoch 20, Loss: 0.630387157201767\n",
      "Epoch 21, Loss: 0.6183584928512573\n",
      "Epoch 22, Loss: 0.6069356799125671\n",
      "Epoch 23, Loss: 0.6266894340515137\n",
      "Epoch 24, Loss: 0.5970357656478882\n",
      "Epoch 25, Loss: 0.6535950005054474\n",
      "Epoch 26, Loss: 0.6264328062534332\n",
      "Epoch 27, Loss: 0.607656866312027\n",
      "Epoch 28, Loss: 0.7094417214393616\n",
      "Epoch 29, Loss: 0.6218211650848389\n",
      "Epoch 30, Loss: 0.6512820720672607\n",
      "Epoch 31, Loss: 0.5807968378067017\n",
      "Epoch 32, Loss: 0.6106638014316559\n",
      "Epoch 33, Loss: 0.6455849409103394\n",
      "Epoch 34, Loss: 0.6457647383213043\n",
      "Epoch 35, Loss: 0.6144834756851196\n",
      "Epoch 36, Loss: 0.6528790593147278\n",
      "Epoch 37, Loss: 0.5683229267597198\n",
      "Epoch 38, Loss: 0.5986946821212769\n",
      "Epoch 39, Loss: 0.5819775462150574\n",
      "Epoch 40, Loss: 0.6097362637519836\n",
      "Epoch 41, Loss: 0.6483001410961151\n",
      "Epoch 42, Loss: 0.6336483061313629\n",
      "Epoch 43, Loss: 0.5893434584140778\n",
      "Epoch 44, Loss: 0.5489798039197922\n",
      "Epoch 45, Loss: 0.6604017615318298\n",
      "Epoch 46, Loss: 0.6441987752914429\n",
      "Epoch 47, Loss: 0.6316342949867249\n",
      "Epoch 48, Loss: 0.5527742654085159\n",
      "Epoch 49, Loss: 0.574690192937851\n",
      "Epoch 50, Loss: 0.6684678792953491\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the Classifier\n",
    "for epoch in range(epochs):\n",
    "    classifier.train()\n",
    "    train_loss = 0\n",
    "    for data, target in encoded_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = classifier(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    print(f'Epoch {epoch+1}, Loss: {train_loss / len(encoded_loader)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7894737124443054\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate the Classifier\n",
    "autoencoder.eval()\n",
    "classifier.eval()\n",
    "with torch.no_grad():\n",
    "    _, encoded_test_features = autoencoder(X_test_tensor)\n",
    "    output = classifier(encoded_test_features)\n",
    "    _, predicted = torch.max(output, 1)\n",
    "    accuracy = (predicted == y_test_tensor).float().mean().item()\n",
    "    print(f'Test Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7432432174682617\n"
     ]
    }
   ],
   "source": [
    "# train accuracy\n",
    "autoencoder.eval()\n",
    "classifier.eval()\n",
    "with torch.no_grad():\n",
    "    _, encoded_test_features = autoencoder(X_train_tensor)\n",
    "    output = classifier(encoded_test_features)\n",
    "    _, predicted = torch.max(output, 1)\n",
    "    accuracy = (predicted == y_train_tensor).float().mean().item()\n",
    "    print(f'Train Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starts with number, ends with \".nii\"\n",
    "re_number_nii = re.compile(r'^\\d.*\\.nii$')\n",
    "\n",
    "#starts with number\n",
    "re_number = re.compile(r'^\\d.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 002_S_1155_seg8.mat\n",
      "2 003_S_0908_reorient.mat\n",
      "3 003_S_1122_reorient.mat\n",
      "4 003_S_6258_reorient.mat\n",
      "5 003_S_6268_seg8.mat\n",
      "6 003_S_6432_seg8.mat\n",
      "7 003_S_6479_reorient.mat\n",
      "8 003_S_6606_reorient.mat\n",
      "9 012_S_6073_seg8.mat\n",
      "10 012_S_6760_seg8.mat\n",
      "11 013_S_6206_seg8.mat\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlistdir(struct_dir)\n\u001b[0;32m     11\u001b[0m cnt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 12\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[43mfile\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(cnt,f)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# lovda_img = os.path.join(struct_dir,f)\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# struct_image = nib.load(lovda_img)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# features = struct_masker.transform(struct_image)\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# correlation_matrix = correlation_measure.fit_transform([features])[0]\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# struct_features.append(correlation_matrix.reshape(-1))\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# print(cnt,f)\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "            # lovda_img = os.path.join(struct_dir,f)\n",
    "            # struct_image = nib.load(lovda_img)\n",
    "            # features = struct_masker.transform(struct_image)\n",
    "            # correlation_matrix = correlation_measure.fit_transform([features])[0]\n",
    "            # struct_features.append(correlation_matrix.reshape(-1))\n",
    "            # print(cnt,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
