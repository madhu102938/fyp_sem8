{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "y0TyzxthhRch"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-08 19:47:32.118154: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-08 19:47:32.137556: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1741443452.160118 1329386 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1741443452.166772 1329386 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-08 19:47:32.191804: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, ConvLSTM3D, Conv3D, Flatten, Dense, Dropout, BatchNormalization, GlobalAveragePooling3D, Concatenate, TimeDistributed, MaxPooling3D, AveragePooling3D, GlobalMaxPooling3D, LSTM, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "devices = tf.config.list_physical_devices()\n",
    "print(devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing data\n",
    "def returning_pkl_file_data(path : str):\n",
    "    with open(path, 'rb') as f:\n",
    "        temp = pkl.load(f)\n",
    "    return temp\n",
    "\n",
    "mci_func = returning_pkl_file_data(r\"feature_extraction/MCI_func_52_79_95_79_197.pkl\")\n",
    "mci_struct = returning_pkl_file_data(r'feature_extraction/MCI_struct_cat_52_169_205_169.pkl')\n",
    "cn_func = returning_pkl_file_data(r'feature_extraction/CN_func_42_79_95_79_197.pkl')\n",
    "cn_struct = returning_pkl_file_data(r'feature_extraction/CN_struct_cat_42_169_205_169.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94, 79, 95, 79, 197)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func_data = np.concat((mci_func, cn_func), axis=0, dtype=np.float16)\n",
    "func_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94, 169, 205, 169)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct_data = np.concat((mci_struct, cn_struct), axis=0, dtype=np.float16)\n",
    "struct_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_labels = np.concat((np.zeros((len(mci_func),)), np.ones((len(cn_func),))))\n",
    "all_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((94, 79, 95, 79, 197, 1), (94, 169, 205, 169, 1))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func_data = np.expand_dims(func_data, axis=len(func_data.shape))\n",
    "struct_data = np.expand_dims(struct_data, axis=len(struct_data.shape))\n",
    "\n",
    "func_data.shape, struct_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94, 40, 48, 40, 197, 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((94, 40, 48, 40, 197, 1), (94, 85, 102, 85, 1, 1))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from skimage.transform import resize\n",
    "\n",
    "def downsample_volume(volume, new_shape):\n",
    "    return resize(volume, new_shape, mode='constant')\n",
    "\n",
    "# Downsample fMRI data from (79, 95, 79, 197, 1) to (40, 48, 40, 100, 1)\n",
    "new_shape_fmri = (40, 48, 40, 197, 1)\n",
    "new_shape_smri = (85, 102, 85, 1) \n",
    "# func_train.shape, func_test.shape, struct_ train.shape, struct_test.shape, y_train.shape, y_test.shape\n",
    "func_data = np.array([downsample_volume(sample, new_shape_fmri) for sample in func_data])\n",
    "struct_data = np.array([downsample_volume(sample, new_shape_smri) for sample in struct_data])\n",
    "func_data.shape, struct_data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94, 85, 102, 85, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct_data = struct_data.squeeze()\n",
    "struct_data = np.expand_dims(struct_data, axis=-1)\n",
    "struct_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create memory-efficient dataset using generator\n",
    "class BrainDataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, func_data, struct_data, labels, batch_size=4):\n",
    "        self.func_data = func_data\n",
    "        self.struct_data = struct_data\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.indices = np.arange(len(func_data))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.func_data) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch_indices = self.indices[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "        return (\n",
    "            {\n",
    "                'func': self.func_data[batch_indices],\n",
    "                'struct': self.struct_data[batch_indices]\n",
    "            },\n",
    "            self.labels[batch_indices]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split indices to avoid data duplication\n",
    "indices = np.arange(len(func_data))\n",
    "train_idx, test_idx = train_test_split(indices, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create memory-mapped datasets\n",
    "train_generator = BrainDataGenerator(\n",
    "    func_data[train_idx], \n",
    "    struct_data[train_idx], \n",
    "    all_labels[train_idx]\n",
    ")\n",
    "\n",
    "test_generator = BrainDataGenerator(\n",
    "    func_data[test_idx], \n",
    "    struct_data[test_idx], \n",
    "    all_labels[test_idx],\n",
    "    batch_size=len(test_idx)  # Full batch for testing\n",
    ")\n",
    "\n",
    "# 4. Convert to tf.data.Dataset with optimized pipeline\n",
    "def create_tf_dataset(generator, training=True):\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: generator,\n",
    "        output_signature=(\n",
    "            {\n",
    "                'func': tf.TensorSpec(shape=(None, *func_data.shape[1:]), dtype=tf.float16),\n",
    "                'struct': tf.TensorSpec(shape=(None, *struct_data.shape[1:]), dtype=tf.float16)\n",
    "            },\n",
    "            tf.TensorSpec(shape=(None,), dtype=tf.float16)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    if training:\n",
    "        dataset = dataset.repeat()  # Infinite repetition for training\n",
    "        dataset = dataset.shuffle(100)\n",
    "        \n",
    "    dataset = dataset.cache().prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "train_dataset = create_tf_dataset(train_generator)\n",
    "test_dataset = create_tf_dataset(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 863
    },
    "id": "qc5Wcgt0et5F",
    "outputId": "23ec5b44-23f7-4990-e140-155fd7a4feb9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Combined_Model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"Combined_Model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ func (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)       │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ struct (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">85</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">102</span>,   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">85</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)            │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ cast_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Cast</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ func[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)       │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ cast_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Cast</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">85</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">102</span>,   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ struct[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">85</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)            │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>,   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ cast_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)        │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv3d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv3D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">83</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>,   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │ cast_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">83</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_distributed_4  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>,   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │ lambda_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │ <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalization │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">83</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>,   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ conv3d_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">83</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_distributed_5  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>,   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ time_distributed… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │ <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv3d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv3D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">81</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">98</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">27,680</span> │ batch_normalizat… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">81</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_distributed_6  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>,   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">55,360</span> │ time_distributed… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │ <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">81</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">98</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ conv3d_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">81</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_distributed_7  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ time_distributed… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">98,816</span> │ time_distributed… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,224</span> │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ func (\u001b[38;5;33mInputLayer\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m48\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│                     │ \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m1\u001b[0m)       │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ struct (\u001b[38;5;33mInputLayer\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m85\u001b[0m, \u001b[38;5;34m102\u001b[0m,   │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│                     │ \u001b[38;5;34m85\u001b[0m, \u001b[38;5;34m1\u001b[0m)            │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ cast_2 (\u001b[38;5;33mCast\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m48\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ func[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "│                     │ \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m1\u001b[0m)       │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ cast_3 (\u001b[38;5;33mCast\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m85\u001b[0m, \u001b[38;5;34m102\u001b[0m,   │          \u001b[38;5;34m0\u001b[0m │ struct[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│                     │ \u001b[38;5;34m85\u001b[0m, \u001b[38;5;34m1\u001b[0m)            │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_1 (\u001b[38;5;33mLambda\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m40\u001b[0m,   │          \u001b[38;5;34m0\u001b[0m │ cast_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│                     │ \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m1\u001b[0m)        │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv3d_5 (\u001b[38;5;33mConv3D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m83\u001b[0m, \u001b[38;5;34m100\u001b[0m,   │        \u001b[38;5;34m896\u001b[0m │ cast_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│                     │ \u001b[38;5;34m83\u001b[0m, \u001b[38;5;34m32\u001b[0m)           │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_distributed_4  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m40\u001b[0m,   │        \u001b[38;5;34m896\u001b[0m │ lambda_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)   │ \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalization │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m83\u001b[0m, \u001b[38;5;34m100\u001b[0m,   │        \u001b[38;5;34m128\u001b[0m │ conv3d_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m83\u001b[0m, \u001b[38;5;34m32\u001b[0m)           │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_distributed_5  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m20\u001b[0m,   │          \u001b[38;5;34m0\u001b[0m │ time_distributed… │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)   │ \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv3d_6 (\u001b[38;5;33mConv3D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m81\u001b[0m, \u001b[38;5;34m98\u001b[0m,    │     \u001b[38;5;34m27,680\u001b[0m │ batch_normalizat… │\n",
       "│                     │ \u001b[38;5;34m81\u001b[0m, \u001b[38;5;34m32\u001b[0m)           │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_distributed_6  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m20\u001b[0m,   │     \u001b[38;5;34m55,360\u001b[0m │ time_distributed… │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)   │ \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m81\u001b[0m, \u001b[38;5;34m98\u001b[0m,    │        \u001b[38;5;34m128\u001b[0m │ conv3d_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m81\u001b[0m, \u001b[38;5;34m32\u001b[0m)           │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_distributed_7  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ time_distributed… │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)   │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m98,816\u001b[0m │ time_distributed… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │      \u001b[38;5;34m4,224\u001b[0m │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ lstm_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m32,896\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │        \u001b[38;5;34m129\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">221,153</span> (863.88 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m221,153\u001b[0m (863.88 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">221,025</span> (863.38 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m221,025\u001b[0m (863.38 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> (512.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m128\u001b[0m (512.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- fMRI Model (ConvLSTM) ---\n",
    "def build_fmri_model():\n",
    "    fmri_input = Input(shape=func_data.shape[1:], name=\"func\", dtype=tf.float16)\n",
    "    x = Lambda(lambda x: tf.transpose(x, perm=[0, 4, 1, 2, 3, 5]))(fmri_input)\n",
    "    # Apply Conv3D independently to each time step\n",
    "    x = TimeDistributed(Conv3D(32, (3,3,3), padding=\"same\", activation=\"relu\"))(x)\n",
    "    x = TimeDistributed(MaxPooling3D(2))(x)  # Downsample spatial dimensions\n",
    "    x = TimeDistributed(Conv3D(64, (3,3,3), padding=\"same\", activation=\"relu\"))(x)\n",
    "    x = TimeDistributed(GlobalAveragePooling3D())(x)  # Shape: (batch, time=197, 64)\n",
    "    # Temporal modeling with LSTM\n",
    "    x = LSTM(128)(x)  # Output shape: (batch, 128)\n",
    "    return Model(inputs=fmri_input, outputs=x, name=\"fMRI_Model\")\n",
    "\n",
    "# --- sMRI Model (3D CNN) ---\n",
    "def build_smri_model():\n",
    "    smri_input = Input(shape=struct_data.shape[1:], name=\"struct\", dtype=tf.float16)\n",
    "\n",
    "    y = Conv3D(filters=32, kernel_size=(3,3,3), activation=\"relu\", padding=\"valid\")(smri_input)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Conv3D(filters=32, kernel_size=(3,3,3), activation=\"relu\", padding=\"valid\")(y)\n",
    "    y = BatchNormalization()(y)\n",
    "\n",
    "    y = GlobalAveragePooling3D()(y)\n",
    "    y = Dense(128, activation=\"relu\")(y)\n",
    "\n",
    "    return Model(inputs=smri_input, outputs=y, name=\"sMRI_Model\")\n",
    "\n",
    "# --- Combine fMRI & sMRI Models ---\n",
    "def build_combined_model():\n",
    "    fmri_model = build_fmri_model()\n",
    "    smri_model = build_smri_model()\n",
    "\n",
    "    combined = Concatenate()([fmri_model.output, smri_model.output])\n",
    "    combined = Dense(128, activation=\"relu\")(combined)\n",
    "    combined = Dropout(0.5)(combined)\n",
    "    output = Dense(1, activation=\"sigmoid\")(combined)\n",
    "\n",
    "    model = Model(inputs=[fmri_model.input, smri_model.input], outputs=output, name=\"Combined_Model\")\n",
    "    return model\n",
    "\n",
    "# Build & Compile Model\n",
    "model = build_combined_model()\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w5DQyVy7gobu",
    "outputId": "dcffe9f0-8f95-4f7b-b800-8496be868ac7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 5, 4, 4, 4, 1), (100, 10, 10, 10, 1), (100,))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func_data = np.random.rand(100, 5, 4, 4, 4, 1)\n",
    "struct_data = np.random.rand(100, 10, 10, 10, 1)\n",
    "all_labels = np.random.randint(0, 2, size=(100,))\n",
    "func_data.shape, struct_data.shape, all_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JAaAp4ICh_Zu"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5HemPPf5icUx",
    "outputId": "494eb634-c62c-4710-f3a0-7bbeb83046e3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((80, 5, 4, 4, 4, 1),\n",
       " (20, 5, 4, 4, 4, 1),\n",
       " (80, 10, 10, 10, 1),\n",
       " (20, 10, 10, 10, 1),\n",
       " (80,),\n",
       " (20,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_fmri, val_fmri, train_smri, val_smri, train_labels, val_labels = train_test_split(fmri_data, smri_data, labels, test_size=0.2, random_state=42)\n",
    "train_fmri.shape, val_fmri.shape, train_smri.shape, val_smri.shape, train_labels.shape, val_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 550
    },
    "id": "8_aofj-zhnaY",
    "outputId": "92860580-1622-4cef-de33-f8dc36a85992"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-08 21:12:21.746300: E tensorflow/core/util/util.cc:131] oneDNN supports DT_HALF only on platforms with AVX-512. Falling back to the default Eigen-based implementation if present.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1741448550.293230 1330295 service.cc:148] XLA service 0x7475a4006260 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1741448550.293272 1330295 service.cc:156]   StreamExecutor device (0): Host, Default Version\n",
      "2025-03-08 21:12:31.582372: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1741448759.869110 1330295 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2025-03-08 21:16:09.898249: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:497] Allocator (mklcpu) ran out of memory trying to allocate 1.56TiB (rounded to 1717586539520)requested by op \n",
      "2025-03-08 21:16:09.898344: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1053] BFCAllocator dump for mklcpu\n",
      "2025-03-08 21:16:09.898367: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (256): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-03-08 21:16:09.898383: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (512): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-03-08 21:16:09.898400: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (1024): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-03-08 21:16:09.898414: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (2048): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-03-08 21:16:09.898427: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (4096): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-03-08 21:16:09.898441: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (8192): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-03-08 21:16:09.898455: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-03-08 21:16:09.898469: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (32768): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-03-08 21:16:09.898483: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-03-08 21:16:09.898496: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-03-08 21:16:09.898518: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (262144): \tTotal Chunks: 3, Chunks in use: 2. 768.0KiB allocated for chunks. 512.0KiB in use in bin. 512.0KiB client-requested in use in bin.\n",
      "2025-03-08 21:16:09.898534: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (524288): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-03-08 21:16:09.898550: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (1048576): \tTotal Chunks: 1, Chunks in use: 0. 1.25MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-03-08 21:16:09.898564: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (2097152): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-03-08 21:16:09.898583: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (4194304): \tTotal Chunks: 17, Chunks in use: 17. 94.18MiB allocated for chunks. 94.18MiB in use in bin. 94.18MiB client-requested in use in bin.\n",
      "2025-03-08 21:16:09.898597: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-03-08 21:16:09.898611: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-03-08 21:16:09.898625: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-03-08 21:16:09.898642: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (67108864): \tTotal Chunks: 15, Chunks in use: 15. 1.67GiB allocated for chunks. 1.67GiB in use in bin. 1.66GiB client-requested in use in bin.\n",
      "2025-03-08 21:16:09.898660: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (134217728): \tTotal Chunks: 4, Chunks in use: 4. 582.80MiB allocated for chunks. 582.80MiB in use in bin. 461.72MiB client-requested in use in bin.\n",
      "2025-03-08 21:16:09.898670: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (268435456): \tTotal Chunks: 1, Chunks in use: 0. 1.67GiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-03-08 21:16:09.898674: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1076] Bin for 1.56TiB was 256.00MiB, Chunk State: \n",
      "2025-03-08 21:16:09.898681: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1082]   Size: 1.67GiB | Requested Size: 0B | in_use: 0 | bin_num: 20, prev:   Size: 4.22MiB | Requested Size: 4.22MiB | in_use: 1 | bin_num: -1\n",
      "2025-03-08 21:16:09.898685: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1089] Next region of size 2097152\n",
      "2025-03-08 21:16:09.898690: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 5f35d24ccf00 of size 262144 next 1\n",
      "2025-03-08 21:16:09.898696: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] Free  at 5f35d250cf00 of size 262144 next 2\n",
      "2025-03-08 21:16:09.898699: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 5f35d254cf00 of size 262144 next 3\n",
      "2025-03-08 21:16:09.898702: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] Free  at 5f35d258cf00 of size 1310720 next 18446744073709551615\n",
      "2025-03-08 21:16:09.898705: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1089] Next region of size 2147483648\n",
      "2025-03-08 21:16:09.898708: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 7470c9a00040 of size 5895680 next 34\n",
      "2025-03-08 21:16:09.898711: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 7470c9f9f640 of size 121036800 next 35\n",
      "2025-03-08 21:16:09.898714: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 7470d130d640 of size 5895680 next 36\n",
      "2025-03-08 21:16:09.898717: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 7470d18acc40 of size 121036800 next 37\n",
      "2025-03-08 21:16:09.898719: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 7470d8c1ac40 of size 5895680 next 38\n",
      "2025-03-08 21:16:09.898722: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 7470d91ba240 of size 90777600 next 39\n",
      "2025-03-08 21:16:09.898725: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 7470de84ca40 of size 4421888 next 40\n",
      "2025-03-08 21:16:09.898728: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] Free  at 7470dec84340 of size 1792523520 next 18446744073709551615\n",
      "2025-03-08 21:16:09.898731: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1089] Next region of size 1073741824\n",
      "2025-03-08 21:16:09.898734: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 747149c00040 of size 5895680 next 19\n",
      "2025-03-08 21:16:09.898760: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 74714a19f640 of size 121036800 next 20\n",
      "2025-03-08 21:16:09.898763: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 74715150d640 of size 5895680 next 21\n",
      "2025-03-08 21:16:09.898766: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 747151aacc40 of size 121036800 next 22\n",
      "2025-03-08 21:16:09.898769: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 747158e1ac40 of size 5895680 next 23\n",
      "2025-03-08 21:16:09.898771: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 7471593ba240 of size 121036800 next 24\n",
      "2025-03-08 21:16:09.898774: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 747160728240 of size 5895680 next 25\n",
      "2025-03-08 21:16:09.898777: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 747160cc7840 of size 121036800 next 26\n",
      "2025-03-08 21:16:09.898780: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 747168035840 of size 5895680 next 27\n",
      "2025-03-08 21:16:09.898783: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 7471685d4e40 of size 121036800 next 28\n",
      "2025-03-08 21:16:09.898785: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 74716f942e40 of size 5895680 next 29\n",
      "2025-03-08 21:16:09.898788: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 74716fee2440 of size 121036800 next 30\n",
      "2025-03-08 21:16:09.898791: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 747177250440 of size 121036800 next 31\n",
      "2025-03-08 21:16:09.898794: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 74717e5be440 of size 5895680 next 32\n",
      "2025-03-08 21:16:09.898797: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 74717eb5da40 of size 185214464 next 18446744073709551615\n",
      "2025-03-08 21:16:09.898799: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1089] Next region of size 536870912\n",
      "2025-03-08 21:16:09.898802: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 747189e00040 of size 5895680 next 11\n",
      "2025-03-08 21:16:09.898805: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 74718a39f640 of size 121036800 next 12\n",
      "2025-03-08 21:16:09.898808: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 74719170d640 of size 5895680 next 13\n",
      "2025-03-08 21:16:09.898811: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 747191cacc40 of size 121036800 next 14\n",
      "2025-03-08 21:16:09.898813: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 74719901ac40 of size 5895680 next 15\n",
      "2025-03-08 21:16:09.898816: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 7471995ba240 of size 121036800 next 16\n",
      "2025-03-08 21:16:09.898819: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 7471a0928240 of size 5895680 next 17\n",
      "2025-03-08 21:16:09.898822: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 7471a0ec7840 of size 150177792 next 18446744073709551615\n",
      "2025-03-08 21:16:09.898824: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1089] Next region of size 268435456\n",
      "2025-03-08 21:16:09.898827: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 7471aa000040 of size 121036800 next 8\n",
      "2025-03-08 21:16:09.898830: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 7471b136e040 of size 5895680 next 9\n",
      "2025-03-08 21:16:09.898833: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 7471b190d640 of size 141502976 next 18446744073709551615\n",
      "2025-03-08 21:16:09.898836: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1089] Next region of size 134217728\n",
      "2025-03-08 21:16:09.898839: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 7471ba200040 of size 5895680 next 6\n",
      "2025-03-08 21:16:09.898842: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 7471ba79f640 of size 128322048 next 18446744073709551615\n",
      "2025-03-08 21:16:09.898844: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1089] Next region of size 134217728\n",
      "2025-03-08 21:16:09.898848: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 7471cfe00040 of size 134217728 next 18446744073709551615\n",
      "2025-03-08 21:16:09.898850: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1114]      Summary of in-use Chunks by size: \n",
      "2025-03-08 21:16:09.898855: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 2 Chunks of size 262144 totalling 512.0KiB\n",
      "2025-03-08 21:16:09.898861: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 1 Chunks of size 4421888 totalling 4.22MiB\n",
      "2025-03-08 21:16:09.898864: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 16 Chunks of size 5895680 totalling 89.96MiB\n",
      "2025-03-08 21:16:09.898867: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 1 Chunks of size 90777600 totalling 86.57MiB\n",
      "2025-03-08 21:16:09.898871: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 13 Chunks of size 121036800 totalling 1.46GiB\n",
      "2025-03-08 21:16:09.898876: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 1 Chunks of size 128322048 totalling 122.38MiB\n",
      "2025-03-08 21:16:09.898879: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 1 Chunks of size 134217728 totalling 128.00MiB\n",
      "2025-03-08 21:16:09.898882: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 1 Chunks of size 141502976 totalling 134.95MiB\n",
      "2025-03-08 21:16:09.898886: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 1 Chunks of size 150177792 totalling 143.22MiB\n",
      "2025-03-08 21:16:09.898889: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 1 Chunks of size 185214464 totalling 176.63MiB\n",
      "2025-03-08 21:16:09.898892: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1121] Sum Total of in-use chunks: 2.33GiB\n",
      "2025-03-08 21:16:09.898895: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1123] Total bytes in pool: 4297064448 memory_limit_: 540659363840 available bytes: 536362299392 curr_region_allocation_bytes_: 4294967296\n",
      "2025-03-08 21:16:09.898904: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1128] Stats: \n",
      "Limit:                    540659363840\n",
      "InUse:                      2502968064\n",
      "MaxInUse:                   2502968064\n",
      "NumAllocs:                          52\n",
      "MaxAllocSize:                185214464\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2025-03-08 21:16:09.898911: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:508] *********_________________________________________************************x*************************\n",
      "2025-03-08 21:16:09.899276: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 1717586539408 bytes.\n",
      "\t [[{{node StatefulPartitionedCall}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n",
      "\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n\n  File \"/tmp/ipykernel_1329386/699743898.py\", line 1, in <module>\n\n  File \"/home/tripti/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/tripti/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 368, in fit\n\n  File \"/home/tripti/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 216, in function\n\n  File \"/home/tripti/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 129, in multi_step_on_iterator\n\nOut of memory while trying to allocate 1717586539408 bytes.\n\t [[{{node StatefulPartitionedCall}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_multi_step_on_iterator_44208]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n\n  File \"/tmp/ipykernel_1329386/699743898.py\", line 1, in <module>\n\n  File \"/home/tripti/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/tripti/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 368, in fit\n\n  File \"/home/tripti/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 216, in function\n\n  File \"/home/tripti/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 129, in multi_step_on_iterator\n\nOut of memory while trying to allocate 1717586539408 bytes.\n\t [[{{node StatefulPartitionedCall}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_multi_step_on_iterator_44208]"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=test_dataset,\n",
    "    epochs=10,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    validation_steps=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
